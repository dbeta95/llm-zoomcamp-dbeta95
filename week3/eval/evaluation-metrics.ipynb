{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "There are different ways of performing information retrieval (vector search, semantic search, etc.) and the best case depends on the data and requirements, which means we need to have a way to know for a certain escenario what is the best. Therefore we use evaluation.\n",
    "\n",
    "The general idea is tha wee need to have a query and the knowledge of the most relevant information, which should be retrieved and we can use that to know how the retrieval techniques are performing.\n",
    "\n",
    "## Evaluation metrics\n",
    "\n",
    "These are formulas to compute a particular number to measure the performance of a particular retrieval method. It also allows to tune the parameters of the query for each technique\n",
    "\n",
    "In information retrieval and machine learning, common ranking evaluation metrics are used to assess the effectiveness of a ranking algorithm. Some of the most widely used metrics are:\n",
    "\n",
    "**1. Precision at k (P@k):**\n",
    "\n",
    "* Measures the numberof relevant documents in the top k results.\n",
    "* Formula -> $P@k = \\frac{\\textrm{Number of relevant documents in top k results}}{k}$\n",
    "\n",
    "\n",
    "**2. Recall:**\n",
    "\n",
    "* Measures the number of relevant documentrs rettieved out of the total number of relevant documents available\n",
    "* Formula -> $Recall = \\frac{Number of relevant docuents retrieved}{Total number of relevant documents}$\n",
    "\n",
    "**3. Mean Average Precision (MAP):**\n",
    "\n",
    "* Computes the average precision for each query and then averages these values over all queries\n",
    "* Formula -> $MAP = \\frac{1}{|Q|} \\sum_{q\\in Q} Average Precision (q)$\n",
    "\n",
    "\n",
    "**4. Normalized  Discounted Cumulative Gain (NDCG):**\n",
    "\n",
    "* Measuresthe usefulness, or gain, of a document based on its position in the result list\n",
    "* Formula -> $NDCG = \\frac{DCG}{IDCG}$\n",
    "    * $DCG = \\sum_{i=1}^p \\frac{2^{rel_i}-1}{log_2(i+1)}$\n",
    "    * IDGC is the ideal DGC, where documents are perfectly ranked by relevance\n",
    "\n",
    "    \n",
    "**5. Mean Reciprocal Rank (MRR):**\n",
    "\n",
    "* Evaluates the rank position of the first relevant document.\n",
    "* Formula -> $MMR= \\frac{1}{|Q|} \\sum_{i=1}^{|Q|}\\frac{1}{rank_i}$\n",
    "\n",
    "\n",
    "6. **F1 Score**:\n",
    "   - Harmonic mean of precision and recall.\n",
    "   - Formula: $F1 = 2 * (Precision * Recall) / (Precision + Recall)$\n",
    "\n",
    "7. **Area Under the ROC Curve (AUC-ROC)**:\n",
    "   - Measures the ability of the model to distinguish between relevant and non-relevant documents.\n",
    "   - AUC is the area under the Receiver Operating Characteristic (ROC) curve, which plots true positive rate (TPR) against false positive rate (FPR).\n",
    "\n",
    "8. **Mean Rank (MR)**:\n",
    "   - The average rank of the first relevant document across all queries.\n",
    "   - Lower values indicate better performance.\n",
    "\n",
    "9. **Hit Rate (HR) or Recall at k**:\n",
    "   - Measures the proportion of queries for which at least one relevant document is retrieved in the top k results.\n",
    "   - Formula: $HR@k = (Number of queries with at least one relevant document in top k) / |Q|$\n",
    "\n",
    "10. **Expected Reciprocal Rank (ERR)**:\n",
    "    - Measures the probability that a user finds a relevant document at each position in the ranked list, assuming a cascading model of user behavior.\n",
    "    - Formula: $ERR = Î£ (1 / i) * \\pi (1 - r_j) * r_i$ for j = 1 to i-1\n",
    "      - Where $r_i$ is the relevance probability of the document at position i."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-zoomcampt-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
