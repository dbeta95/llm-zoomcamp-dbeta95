{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18bb7217-ffd0-43de-af05-69288860cefe",
   "metadata": {},
   "source": [
    "# Flan t5\n",
    "\n",
    "in this seciton we explore the first open source LLM to use as an alternative to GPT using Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7073a8-cfcb-491a-91ef-d19223e81f7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:28:32.766751Z",
     "iopub.status.busy": "2024-07-02T03:28:32.766432Z",
     "iopub.status.idle": "2024-07-02T03:28:33.417988Z",
     "shell.execute_reply": "2024-07-02T03:28:33.417108Z",
     "shell.execute_reply.started": "2024-07-02T03:28:32.766724Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay         100G   37G   64G  37% /\n",
      "tmpfs            64M     0   64M   0% /dev\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1  100G   37G   64G  37% /run\n",
      "tmpfs            14G     0   14G   0% /dev/shm\n",
      "/dev/nvme2n1    2.0G  149M  1.8G   8% /home/jovyan\n",
      "tmpfs            14G  120K   14G   1% /home/jovyan/.saturn\n",
      "tmpfs            14G   12K   14G   1% /run/secrets/kubernetes.io/serviceaccount\n",
      "tmpfs           7.7G   12K  7.7G   1% /proc/driver/nvidia\n",
      "tmpfs           7.7G  6.6M  7.7G   1% /run/nvidia-persistenced/socket\n",
      "tmpfs           7.7G     0  7.7G   0% /proc/acpi\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2178ab32-73f4-4abb-8da0-484f6d91d90c",
   "metadata": {},
   "source": [
    "As we se above the home directory has very little space available, so we are going to define a different folder for the model's files to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c2c263-10af-4d1c-add2-0efe9583eada",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:27:49.559570Z",
     "iopub.status.busy": "2024-07-02T03:27:49.559190Z",
     "iopub.status.idle": "2024-07-02T03:27:49.567053Z",
     "shell.execute_reply": "2024-07-02T03:27:49.566028Z",
     "shell.execute_reply.started": "2024-07-02T03:27:49.559542Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/run/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43572873-d20a-400e-bb8f-2b082592bd55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:29:16.383124Z",
     "iopub.status.busy": "2024-07-02T03:29:16.382699Z",
     "iopub.status.idle": "2024-07-02T03:29:23.600823Z",
     "shell.execute_reply": "2024-07-02T03:29:23.600032Z",
     "shell.execute_reply.started": "2024-07-02T03:29:16.383092Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3389082-545e-4bf5-873d-27aa24d3d940",
   "metadata": {},
   "source": [
    "Now we download the model and files and load them to our model and tockenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ec626c-a6b5-42c0-9000-51029d0a0091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:29:23.603122Z",
     "iopub.status.busy": "2024-07-02T03:29:23.602496Z",
     "iopub.status.idle": "2024-07-02T03:30:41.210206Z",
     "shell.execute_reply": "2024-07-02T03:30:41.209454Z",
     "shell.execute_reply.started": "2024-07-02T03:29:23.603076Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d20ca9e8904c75bf3569dc277863e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59852bd5f48c43309bd946d687a080d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05409eceb1c494f978d5ee26f3c2af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8b53da4c06462dbef9ba3bb1a42418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93519745ef34c16a275f1026ef60e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feefa20f1efc4e5d8201a18be3e4213e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5747dadb1d3e4435ad54d1d59624c228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f5bf6e4c134ce692e7ba2990b07170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc885d289524cd59b483244e5b21714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801acba346e14ae8b0b85c45b13e6069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7842ead4aa4d6cbcec6866e94c84ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f0734b-e697-4c96-95e2-62053e804c24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:33:08.403578Z",
     "iopub.status.busy": "2024-07-02T03:33:08.403156Z",
     "iopub.status.idle": "2024-07-02T03:33:08.409427Z",
     "shell.execute_reply": "2024-07-02T03:33:08.408321Z",
     "shell.execute_reply.started": "2024-07-02T03:33:08.403551Z"
    },
    "tags": []
   },
   "source": [
    "now we use our tockenizer to turns the text into a vector representation that we pass to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9de55fb4-764e-4563-a56f-a60477f4162f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:33:27.160354Z",
     "iopub.status.busy": "2024-07-02T03:33:27.159917Z",
     "iopub.status.idle": "2024-07-02T03:33:27.174657Z",
     "shell.execute_reply": "2024-07-02T03:33:27.173596Z",
     "shell.execute_reply.started": "2024-07-02T03:33:27.160325Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13959,  1566,    12,  2968,    10,   571,   625,    33,    25,    58,\n",
       "             1]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2e315c6-9997-43bb-8d04-902a93c6e820",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:33:48.522992Z",
     "iopub.status.busy": "2024-07-02T03:33:48.522576Z",
     "iopub.status.idle": "2024-07-02T03:33:50.005895Z",
     "shell.execute_reply": "2024-07-02T03:33:50.004895Z",
     "shell.execute_reply.started": "2024-07-02T03:33:48.522963Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Wie alt sind Sie?</s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19157307-dc58-424e-983c-35115f1922b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:50:35.488121Z",
     "iopub.status.busy": "2024-07-02T03:50:35.487657Z",
     "iopub.status.idle": "2024-07-02T03:50:36.036943Z",
     "shell.execute_reply": "2024-07-02T03:50:36.036023Z",
     "shell.execute_reply.started": "2024-07-02T03:50:35.488093Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cu√°ntos aos tiene?\n"
     ]
    }
   ],
   "source": [
    "input_text = \"translate English to Spanish: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f8eed98-a56c-4701-a70a-c1b19b4740fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:41:14.108811Z",
     "iopub.status.busy": "2024-07-02T03:41:14.108398Z",
     "iopub.status.idle": "2024-07-02T03:41:14.342569Z",
     "shell.execute_reply": "2024-07-02T03:41:14.341831Z",
     "shell.execute_reply.started": "2024-07-02T03:41:14.108782Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(input_ids)\n",
    "result = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8c5810-06e4-4641-936a-4f0726d91be1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:41:17.899782Z",
     "iopub.status.busy": "2024-07-02T03:41:17.899196Z",
     "iopub.status.idle": "2024-07-02T03:41:17.906948Z",
     "shell.execute_reply": "2024-07-02T03:41:17.905674Z",
     "shell.execute_reply.started": "2024-07-02T03:41:17.899725Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Wie alt sind Sie?</s>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf0625f6-1185-4e48-9ff2-f93e7afc56f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:43:54.342550Z",
     "iopub.status.busy": "2024-07-02T03:43:54.342142Z",
     "iopub.status.idle": "2024-07-02T03:43:58.274592Z",
     "shell.execute_reply": "2024-07-02T03:43:58.273844Z",
     "shell.execute_reply.started": "2024-07-02T03:43:54.342523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)\n",
    "\n",
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d30d5ef-e9c1-45cb-9dbe-ddf640408a4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:49:21.569111Z",
     "iopub.status.busy": "2024-07-02T03:49:21.568692Z",
     "iopub.status.idle": "2024-07-02T03:49:21.576507Z",
     "shell.execute_reply": "2024-07-02T03:49:21.575303Z",
     "shell.execute_reply.started": "2024-07-02T03:49:21.569083Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt, generate_params=None):\n",
    "    if generate_params is None:\n",
    "        generate_params = {}\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=generate_params.get(\"max_length\", 100),\n",
    "        num_beams=generate_params.get(\"num_beams\", 5),\n",
    "        do_sample=generate_params.get(\"do_sample\", False),\n",
    "        temperature=generate_params.get(\"temperature\", 1.0),\n",
    "        top_k=generate_params.get(\"top_k\", 50),\n",
    "        top_p=generate_params.get(\"top_p\", 0.95),\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74de7aa8-01f5-4d60-a0b2-6b5e559b1b3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:49:22.155182Z",
     "iopub.status.busy": "2024-07-02T03:49:22.154754Z",
     "iopub.status.idle": "2024-07-02T03:49:22.159654Z",
     "shell.execute_reply": "2024-07-02T03:49:22.158696Z",
     "shell.execute_reply.started": "2024-07-02T03:49:22.155155Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b06f74e-4dd7-48e5-b14f-3c6c991ab2c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:49:22.517055Z",
     "iopub.status.busy": "2024-07-02T03:49:22.516667Z",
     "iopub.status.idle": "2024-07-02T03:49:26.273224Z",
     "shell.execute_reply": "2024-07-02T03:49:26.272228Z",
     "shell.execute_reply.started": "2024-07-02T03:49:22.517027Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"I just discovered the course. Can I still join it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a75573f-11a0-46dd-aa0b-487be658f6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
