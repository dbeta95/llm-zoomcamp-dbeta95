{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open source data ingestion for RAGs with dlt\n",
    "\n",
    "## What is data ingestion\n",
    "\n",
    "The porecess of extracting data form a producer, transporting it to a convenient environment and preparing it for usage by normalizing it, sometimes cleaning and adding metadata.\n",
    "\n",
    "Sometimes the format in which it appears is structured, and with an explicit schema (e.g. Parquet or a db table)\n",
    "\n",
    "Most of the time, the format is wakly typed and without explicit schema (e.g. csv and json), in which case some normalization and cleaning is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not justuse some python scripts?\n",
    "\n",
    "* Less work, nomore breaking python scripts\n",
    "* Make sure your data is of good quality\n",
    "* Keep your data up to date without having to reload the entire dataset\n",
    "* Make your data pipelines production ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition considerations for RAGs\n",
    "\n",
    "* Pre-processing:\n",
    "    * Extracting the data form PDFs, json, etc.\n",
    "    * Making sure the resulting text data is clean\n",
    "\n",
    "* Chunking: Making sure the text is in manageable segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dlt (data load tool)\n",
    "\n",
    "dlt is a Python library that automates data loading with features like schema creation, normalization and integration adaptability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How doe sit address our challenges?\n",
    "\n",
    "* Messy python scripts: with dlt you write minimal python doce, it handles most complexities automatically\n",
    "* Extracting the data: dlt automatically unnest jsons, types it and figures out the schema\n",
    "* Data versioning: Each load has an in, is verioned and could be rolled back\n",
    "* Data quality: You can define \"data contracts\" that reject data that isn't of the right type\n",
    "* Scaling:\n",
    "    * Incremental loading - only load new or changed data\n",
    "    * Performance management\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with vector DBs\n",
    "\n",
    "* Most vector DBs only store embeddings and their metadata\n",
    "* Extra infrastructur and maintenance costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing LanceDB: a scalable open source vector DB\n",
    "\n",
    "* Stores your data (including text and images), the embeddings and metadata.\n",
    "* Highly scalableand fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load teh data\n",
    "\n",
    "We'll first load the data just into LanceDB, without embedding it. LanveDB stores both the data and the embeddings, and can also embed data and queries on the fly.\n",
    "\n",
    "Somedefinitions:\n",
    "\n",
    "* A dlt source is a grouping of resources (e.g all your data from Hubspot)\n",
    "* A dlt resource is a function that yields data (e.g. a function yielding all your Hubspot companies)\n",
    "* A dlt pipeline is how you ingest your data\n",
    "\n",
    "Loading the data consists of a few steps:\n",
    "\n",
    "1. Use the requests library to get the data\n",
    "2. Define a dlt resource that yields the individual documents\n",
    "3. Create a dlt pipeline and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline from_json load step completed in 0.22 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset qanda\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x0000022B64C87B20> location to store data\n",
      "Load package 1721688228.8193974 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import dlt\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "qa_dataset = docs_response.json()\n",
    "\n",
    "@dlt.resource\n",
    "def qa_documents():\n",
    "    for course in qa_dataset:\n",
    "        yield course['documents']\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"from_json\", destination=\"lancedb\", dataset_name=\"qanda\"\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(qa_documents, table_name=\"docuements\")\n",
    "\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qanda____dlt_loads', 'qanda____dlt_pipeline_state', 'qanda____dlt_version', 'qanda___dltSentinelTable', 'qanda___docuements']\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"./.lancedb/\")\n",
    "print(db.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__</th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ad19c050-006d-54ab-bed2-66d3aef1edb3</td>\n",
       "      <td>The purpose of this document is to capture fre...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - When will the course start?</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>jVfpGKVDRSSQzQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e0444f5f-573f-5fab-a556-b72524323b40</td>\n",
       "      <td>GitHub - DataTalksClub data-engineering-zoomca...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What are the prerequisites for this c...</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>AptFLPZ8mv0zAw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ffd85169-2f5e-5bc0-9a36-8168b3bbe658</td>\n",
       "      <td>Yes, even if you don't register, you're still ...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - Can I still join the course after the...</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>6w9DaYv1qUjkfw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d32f47cf-fb2e-55cc-a1f3-b6d0d7a30bf5</td>\n",
       "      <td>You don't need it. You're accepted. You can al...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - I have registered for the Data Engine...</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>/rPqEZyAd6bAgQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3c26c7cb-f0c2-5fc1-b6c3-7e8389b45412</td>\n",
       "      <td>You can start by installing and setting up all...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What can I do before the course starts?</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>61Y1mjRbuoC66A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>13c7ef59-421e-5d2c-b8af-b1ff08b7ff50</td>\n",
       "      <td>Problem description\\nThis is the step in the c...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Github actions: Permission denied error when e...</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>7q5XmdXsz1MZQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>6a5e3836-38ee-57b1-a2e9-8dfefc434c80</td>\n",
       "      <td>Problem description\\nWhen a docker-compose fil...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Managing Multiple Docker Containers with docke...</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>OiU7xyT6UcDUlQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>cc8bf787-1ce9-54bc-ac26-13a909e4c763</td>\n",
       "      <td>Problem description\\nIf you are having problem...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>AWS regions need to match docker-compose</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>6JBNh73u1TiDCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>d756443c-f943-5cbf-a0e8-0ea7719d20d5</td>\n",
       "      <td>Problem description\\nPre-commit command was fa...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Isort Pre-commit</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>XAOmTIEcF78tEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>da787a9f-fc71-56f4-a32d-25be44e6ad60</td>\n",
       "      <td>Problem description\\nInfrastructure created in...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>How to destroy infrastructure created via GitH...</td>\n",
       "      <td>1721671783.8362322</td>\n",
       "      <td>MK/sH+u7CylUHQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>948 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id__  \\\n",
       "0    ad19c050-006d-54ab-bed2-66d3aef1edb3   \n",
       "1    e0444f5f-573f-5fab-a556-b72524323b40   \n",
       "2    ffd85169-2f5e-5bc0-9a36-8168b3bbe658   \n",
       "3    d32f47cf-fb2e-55cc-a1f3-b6d0d7a30bf5   \n",
       "4    3c26c7cb-f0c2-5fc1-b6c3-7e8389b45412   \n",
       "..                                    ...   \n",
       "943  13c7ef59-421e-5d2c-b8af-b1ff08b7ff50   \n",
       "944  6a5e3836-38ee-57b1-a2e9-8dfefc434c80   \n",
       "945  cc8bf787-1ce9-54bc-ac26-13a909e4c763   \n",
       "946  d756443c-f943-5cbf-a0e8-0ea7719d20d5   \n",
       "947  da787a9f-fc71-56f4-a32d-25be44e6ad60   \n",
       "\n",
       "                                                  text  \\\n",
       "0    The purpose of this document is to capture fre...   \n",
       "1    GitHub - DataTalksClub data-engineering-zoomca...   \n",
       "2    Yes, even if you don't register, you're still ...   \n",
       "3    You don't need it. You're accepted. You can al...   \n",
       "4    You can start by installing and setting up all...   \n",
       "..                                                 ...   \n",
       "943  Problem description\\nThis is the step in the c...   \n",
       "944  Problem description\\nWhen a docker-compose fil...   \n",
       "945  Problem description\\nIf you are having problem...   \n",
       "946  Problem description\\nPre-commit command was fa...   \n",
       "947  Problem description\\nInfrastructure created in...   \n",
       "\n",
       "                              section  \\\n",
       "0    General course-related questions   \n",
       "1    General course-related questions   \n",
       "2    General course-related questions   \n",
       "3    General course-related questions   \n",
       "4    General course-related questions   \n",
       "..                                ...   \n",
       "943          Module 6: Best practices   \n",
       "944          Module 6: Best practices   \n",
       "945          Module 6: Best practices   \n",
       "946          Module 6: Best practices   \n",
       "947          Module 6: Best practices   \n",
       "\n",
       "                                              question        _dlt_load_id  \\\n",
       "0                 Course - When will the course start?  1721671783.8362322   \n",
       "1    Course - What are the prerequisites for this c...  1721671783.8362322   \n",
       "2    Course - Can I still join the course after the...  1721671783.8362322   \n",
       "3    Course - I have registered for the Data Engine...  1721671783.8362322   \n",
       "4     Course - What can I do before the course starts?  1721671783.8362322   \n",
       "..                                                 ...                 ...   \n",
       "943  Github actions: Permission denied error when e...  1721671783.8362322   \n",
       "944  Managing Multiple Docker Containers with docke...  1721671783.8362322   \n",
       "945           AWS regions need to match docker-compose  1721671783.8362322   \n",
       "946                                   Isort Pre-commit  1721671783.8362322   \n",
       "947  How to destroy infrastructure created via GitH...  1721671783.8362322   \n",
       "\n",
       "            _dlt_id  \n",
       "0    jVfpGKVDRSSQzQ  \n",
       "1    AptFLPZ8mv0zAw  \n",
       "2    6w9DaYv1qUjkfw  \n",
       "3    /rPqEZyAd6bAgQ  \n",
       "4    61Y1mjRbuoC66A  \n",
       "..              ...  \n",
       "943  7q5XmdXsz1MZQA  \n",
       "944  OiU7xyT6UcDUlQ  \n",
       "945  6JBNh73u1TiDCA  \n",
       "946  XAOmTIEcF78tEA  \n",
       "947  MK/sH+u7CylUHQ  \n",
       "\n",
       "[948 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_table = db.open_table('qanda___docuements')\n",
    "\n",
    "db_table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and embed the data\n",
    "\n",
    "Now we load the same data again(into a new table), but embed it directly with the lancedb_adapter. This consists of the following steps:\n",
    "\n",
    "1. Define the embedding model to use via ENV variables\n",
    "2. Define a new pipeline to load the same data and embed the \"text\" and \"question\" columns with the lancedb_adapter\n",
    "\n",
    "You can use any embedding model, form open source to OpenAI. We´ve chosen the all-MinLLM-L6-v2 sentence transformer for speed and simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline from_json_embedded load step completed in 12.15 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset qanda_embedded\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x0000022B64095000> location to store data\n",
      "Load package 1721688231.1588693 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from dlt.destinations.adapters import lancedb_adapter\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(os.getenv('DESTINATION__LANCEDB__EMBEDDING_MODEL'))\n",
    "\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"from_json_embedded\",destination=\"lancedb\", dataset_name=\"qanda_embedded\"\n",
    ")\n",
    "\n",
    "load_info = pipeline.run(\n",
    "    lancedb_adapter(qa_documents, embed=[\"text\", \"question\"]),\n",
    "    table_name=\"documents\"\n",
    ")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qanda____dlt_loads', 'qanda____dlt_pipeline_state', 'qanda____dlt_version', 'qanda___dltSentinelTable', 'qanda___docuements', 'qanda_embedded____dlt_loads', 'qanda_embedded____dlt_pipeline_state', 'qanda_embedded____dlt_version', 'qanda_embedded___dltSentinelTable', 'qanda_embedded___documents']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(db.table_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__</th>\n",
       "      <th>vector__</th>\n",
       "      <th>text</th>\n",
       "      <th>section</th>\n",
       "      <th>question</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3e288511-782e-5c54-ad3d-763f2c5fde4d</td>\n",
       "      <td>[-0.0003509373, -0.06201426, -0.037999906, 0.0...</td>\n",
       "      <td>The purpose of this document is to capture fre...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - When will the course start?</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>/Ng7t4wWjfGDwA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f22d4ff0-e8b1-578c-bd48-f67224a1ce3e</td>\n",
       "      <td>[0.020011412, -0.011535535, 0.013017191, -0.00...</td>\n",
       "      <td>GitHub - DataTalksClub data-engineering-zoomca...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What are the prerequisites for this c...</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>8XdgqnlC9z+dMg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8d4ae14e-9fb4-5007-8c83-b67ec1c019a3</td>\n",
       "      <td>[0.014857557, -0.06664991, -0.013571233, 0.023...</td>\n",
       "      <td>Yes, even if you don't register, you're still ...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - Can I still join the course after the...</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>ehMcg+O3Rkz2Ww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2b01c32a-8963-54d5-a120-7cafdc16bf52</td>\n",
       "      <td>[-0.023312038, -0.09461491, 0.056361593, -0.00...</td>\n",
       "      <td>You don't need it. You're accepted. You can al...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - I have registered for the Data Engine...</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>T9mHXEmVMElJBg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55dd8935-427d-51a8-a3e9-c8f4d9966560</td>\n",
       "      <td>[0.026537651, -0.017796684, 0.0021156122, 0.00...</td>\n",
       "      <td>You can start by installing and setting up all...</td>\n",
       "      <td>General course-related questions</td>\n",
       "      <td>Course - What can I do before the course starts?</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>rGPs+X44mZArxw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>ac535b7e-2dcc-5cde-adb5-4b691d0f2c98</td>\n",
       "      <td>[0.016619325, -0.03360315, -0.09334715, -0.021...</td>\n",
       "      <td>Problem description\\nThis is the step in the c...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Github actions: Permission denied error when e...</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>wmf8T1CaQR3PQQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>e5c592a1-ef76-5cce-bcdd-0bed21a92b8a</td>\n",
       "      <td>[0.026872838, -0.0019948885, 0.008369074, -0.0...</td>\n",
       "      <td>Problem description\\nWhen a docker-compose fil...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Managing Multiple Docker Containers with docke...</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>0/EM7VVPwH2A9g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>59d8e9db-98ad-5d81-ae34-d122e585117e</td>\n",
       "      <td>[0.03513754, 0.056265566, 0.024428504, -0.0651...</td>\n",
       "      <td>Problem description\\nIf you are having problem...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>AWS regions need to match docker-compose</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>SL0cx04C3w16IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>744ca1db-43c7-58b4-a93b-f4fe9965ce54</td>\n",
       "      <td>[0.033809815, -0.0031219649, 0.0017484472, 0.0...</td>\n",
       "      <td>Problem description\\nPre-commit command was fa...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>Isort Pre-commit</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>3PZhMRYp9rq1Lw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>fe73198e-86b4-5b60-973c-eb66ed046ba7</td>\n",
       "      <td>[-0.00075232243, 0.004231526, 0.0025023946, -0...</td>\n",
       "      <td>Problem description\\nInfrastructure created in...</td>\n",
       "      <td>Module 6: Best practices</td>\n",
       "      <td>How to destroy infrastructure created via GitH...</td>\n",
       "      <td>1721675036.142532</td>\n",
       "      <td>6qnqWXTNwZTZEQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>948 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id__  \\\n",
       "0    3e288511-782e-5c54-ad3d-763f2c5fde4d   \n",
       "1    f22d4ff0-e8b1-578c-bd48-f67224a1ce3e   \n",
       "2    8d4ae14e-9fb4-5007-8c83-b67ec1c019a3   \n",
       "3    2b01c32a-8963-54d5-a120-7cafdc16bf52   \n",
       "4    55dd8935-427d-51a8-a3e9-c8f4d9966560   \n",
       "..                                    ...   \n",
       "943  ac535b7e-2dcc-5cde-adb5-4b691d0f2c98   \n",
       "944  e5c592a1-ef76-5cce-bcdd-0bed21a92b8a   \n",
       "945  59d8e9db-98ad-5d81-ae34-d122e585117e   \n",
       "946  744ca1db-43c7-58b4-a93b-f4fe9965ce54   \n",
       "947  fe73198e-86b4-5b60-973c-eb66ed046ba7   \n",
       "\n",
       "                                              vector__  \\\n",
       "0    [-0.0003509373, -0.06201426, -0.037999906, 0.0...   \n",
       "1    [0.020011412, -0.011535535, 0.013017191, -0.00...   \n",
       "2    [0.014857557, -0.06664991, -0.013571233, 0.023...   \n",
       "3    [-0.023312038, -0.09461491, 0.056361593, -0.00...   \n",
       "4    [0.026537651, -0.017796684, 0.0021156122, 0.00...   \n",
       "..                                                 ...   \n",
       "943  [0.016619325, -0.03360315, -0.09334715, -0.021...   \n",
       "944  [0.026872838, -0.0019948885, 0.008369074, -0.0...   \n",
       "945  [0.03513754, 0.056265566, 0.024428504, -0.0651...   \n",
       "946  [0.033809815, -0.0031219649, 0.0017484472, 0.0...   \n",
       "947  [-0.00075232243, 0.004231526, 0.0025023946, -0...   \n",
       "\n",
       "                                                  text  \\\n",
       "0    The purpose of this document is to capture fre...   \n",
       "1    GitHub - DataTalksClub data-engineering-zoomca...   \n",
       "2    Yes, even if you don't register, you're still ...   \n",
       "3    You don't need it. You're accepted. You can al...   \n",
       "4    You can start by installing and setting up all...   \n",
       "..                                                 ...   \n",
       "943  Problem description\\nThis is the step in the c...   \n",
       "944  Problem description\\nWhen a docker-compose fil...   \n",
       "945  Problem description\\nIf you are having problem...   \n",
       "946  Problem description\\nPre-commit command was fa...   \n",
       "947  Problem description\\nInfrastructure created in...   \n",
       "\n",
       "                              section  \\\n",
       "0    General course-related questions   \n",
       "1    General course-related questions   \n",
       "2    General course-related questions   \n",
       "3    General course-related questions   \n",
       "4    General course-related questions   \n",
       "..                                ...   \n",
       "943          Module 6: Best practices   \n",
       "944          Module 6: Best practices   \n",
       "945          Module 6: Best practices   \n",
       "946          Module 6: Best practices   \n",
       "947          Module 6: Best practices   \n",
       "\n",
       "                                              question       _dlt_load_id  \\\n",
       "0                 Course - When will the course start?  1721675036.142532   \n",
       "1    Course - What are the prerequisites for this c...  1721675036.142532   \n",
       "2    Course - Can I still join the course after the...  1721675036.142532   \n",
       "3    Course - I have registered for the Data Engine...  1721675036.142532   \n",
       "4     Course - What can I do before the course starts?  1721675036.142532   \n",
       "..                                                 ...                ...   \n",
       "943  Github actions: Permission denied error when e...  1721675036.142532   \n",
       "944  Managing Multiple Docker Containers with docke...  1721675036.142532   \n",
       "945           AWS regions need to match docker-compose  1721675036.142532   \n",
       "946                                   Isort Pre-commit  1721675036.142532   \n",
       "947  How to destroy infrastructure created via GitH...  1721675036.142532   \n",
       "\n",
       "            _dlt_id  \n",
       "0    /Ng7t4wWjfGDwA  \n",
       "1    8XdgqnlC9z+dMg  \n",
       "2    ehMcg+O3Rkz2Ww  \n",
       "3    T9mHXEmVMElJBg  \n",
       "4    rGPs+X44mZArxw  \n",
       "..              ...  \n",
       "943  wmf8T1CaQR3PQQ  \n",
       "944  0/EM7VVPwH2A9g  \n",
       "945  SL0cx04C3w16IQ  \n",
       "946  3PZhMRYp9rq1Lw  \n",
       "947  6qnqWXTNwZTZEQ  \n",
       "\n",
       "[948 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_table = db.open_table('qanda_embedded___documents')\n",
    "\n",
    "db_table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an up to date RAG with dlt and LanceDB\n",
    "\n",
    "In this demo, we will create a LLM chat bot that has the latest knowledge of the employee handbook of a finctional company. We will be able to chat to it about specific policies like PTO, work from gome etc.\n",
    "\n",
    "To build this we need to od three hings.\n",
    "\n",
    "1. The company policies exist i an Notion Page. We will need first to extract the text from these pages.\n",
    "2. Once extracted, we will want to embed them into vectors and then store them in a vector datacase.\n",
    "3. This will allow us to create our RAG: a function that would accept a user question, mach it to the informatio stored in the vector database, and \n",
    "\n",
    "then send the question + relevant information as input to the LLM.\n",
    "\n",
    "we will be using the following OSS tools for this:\n",
    "\n",
    "1. dlt for data ingestion:\n",
    "\n",
    "    * dlt can easily connecto to any REST_API source (like Notion)\n",
    "    * it also has integrations with vector databases, like LanceDB\n",
    "    * I also allows to easily plug in functionality like incremental loading\n",
    "\n",
    "2. lanceDB as a vector databse:\n",
    "    * LanceDB is an open source vector database that is very easy to use and integrate into python workflows\n",
    "    * It is in-process an serverless (like DuckDB), which makes querying and retrieval veru effcient\n",
    "\n",
    "3. Ollama for RAG:\n",
    "    * Ollama is open source and allows you to easily tun LLMs locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Create a Notion -> LanceDB pipeline using dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a dlt project with rest_api source and lancedb destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a dlt project using the command `dlt init <source> <destination>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This downloads all the modules required for the dlt source (rest api, in this case) into the local directory. See the side panel for the directory structure created.\n",
    "\n",
    "What is the dlt rest api source?\n",
    "\n",
    "It is a dlt source that allows you to connect to any REST API endpoint using a declarative configuration. You can:\n",
    "- pass the endpoints that you want to connect to,\n",
    "- define the relation between the endpoints\n",
    "- define how you want to handle pagination and authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Versi�n 10.0.19045.4651]\n",
      "(c) Microsoft Corporation. Todos los derechos reservados.\n",
      "\n",
      "(llm-zoomcampt-3.10) c:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\homeworks>dlt init rest_api lancedb \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking up the init scripts in https://github.com/dlt-hub/verified-sources.git...\n",
      "Cloning and configuring a verified source rest_api (Generic API Source)\n",
      "Do you want to proceed? [Y/n]: \n",
      "Verified source rest_api was added to your project!\n",
      "* See the usage examples and code snippets to copy from rest_api_pipeline.py\n",
      "* Add credentials for lancedb and other secrets in .\\.dlt\\secrets.toml\n",
      "* requirements.txt was created. Install it with:\n",
      "pip3 install -r requirements.txt\n",
      "* Read https://dlthub.com/docs/walkthroughs/create-a-pipeline for more information\n",
      "\n",
      "(llm-zoomcampt-3.10) c:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\homeworks>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "dlt init rest_api lancedb \n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add API credentials\n",
    "\n",
    "To access APIs, databases, or any third-party applications, one might need to specify relevant credentials.\n",
    "\n",
    "With dlt, we can do it in two ways:\n",
    "\n",
    "Pass the credentials and any other sensitive information inside .dlt/secrets.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sources.rest_api.notion]\n",
    "api_key = \"notion api key\"\n",
    "\n",
    "[destination.lancedb]\n",
    "embedding_model_provider = \"sentence-transformers\"\n",
    "embedding_model = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "[destination.lancedb.credentials]\n",
    "uri = \".lancedb\"\n",
    "api_key = \"api_key\"\n",
    "embedding_model_provider_api_key = \"embedding_model_provider_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass them as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SOURCES__REST_API__NOTION__API_KEY\"] = \"notion api key\"\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL_PROVIDER\"] = \"sentence-transformers\"\n",
    "os.environ[\"DESTINATION__LANCEDB__EMBEDDING_MODEL\"] = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__URI\"] = \".lancedb\"\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__API_KEY\"] = \"api_key\"\n",
    "os.environ[\"DESTINATION__LANCEDB__CREDENTIALS__EMBEDDING_MODEL_PROVIDER_API_KEY\"] = \"embedding_model_provider_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using option 2. It's not advisable to paste sensitive information like API keys inside the code, so instead we're going to include them inside the secrets tab in the side panel of the notebook. This will allow us to access the secret values from the notebook.\n",
    "\n",
    "Since we are using the OSS version of LanceDB and OSS embedding models, we only need to specify the API key for Notion.\n",
    "\n",
    "Note: You will need to copy the notion API key into the secrets tab under the name SOURCES__REST_API__NOTION__API_KEY. Make sure to enable notebook access after pasting the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the env variables\n",
    "load_dotenv(override=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write the pipeline code\n",
    "\n",
    "**Note:** We first go over the code step by step before putting it into runnable cells\n",
    "\n",
    "1. Import necessary modules (run this cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from rest_api import RESTAPIConfig, rest_api_source\n",
    "\n",
    "from dlt.sources.helpers.rest_client.paginators import BasePaginator, JSONResponsePaginator\n",
    "from dlt.sources.helpers.requests import Response, Request\n",
    "\n",
    "from dlt.destinations.adapters import lancedb_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Configure the dlt rest api source to connect to and extract the relevant data out from the Notion Rest API\n",
    "\n",
    "    Our notion space has multiple pages and each page ahs multiple paragraphs (called blocks). To extract all this data form the Notion API, we would first need to get a list of all the page_ids (each page has a unique page_id), and then use the page_id to request the contents from the individual pages. Specifically:\n",
    "\n",
    "    1. We will first request the page_ids from the /serach endpoint\n",
    "    2. And then using the return page_ids, we will request the contents form the /blocks/{page_id}/children endpoint.\n",
    "\n",
    "    With this in mind, we can configure the dlt notion rest api source as follows:\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESTAPIConfig = {\n",
    "    \"client\":{\n",
    "        \"base_url\": \"https://api.notion.com/v1/\",\n",
    "        \"auth\": {\n",
    "            \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "        },\n",
    "        \"headers\": {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Notion-Version\": \"2022-06-28\"\n",
    "        }\n",
    "    },\n",
    "    \"resources\": [\n",
    "        {\n",
    "            \"name\": \"search\",\n",
    "            \"endpoint\": {\n",
    "                \"path\": \"search\",\n",
    "                \"method\": \"POST\",\n",
    "                \"paginator\": PostBodyPaginator(),\n",
    "                \"json\": {\n",
    "                    \"query\": \"workshop\",\n",
    "                    \"sort\": {\n",
    "                        \"direction\": \"ascending\",\n",
    "                        \"timestamp\": \"last_edited_time\"\n",
    "                    }\n",
    "                },\n",
    "                \"data_selector\": \"results\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"page_content\",\n",
    "            \"endpoint\": {\n",
    "                \"path\": \"blocks/{page_id}/children\",\n",
    "                \"paginator\": JSONResponsePaginator(),\n",
    "                \"params\": {\n",
    "                    \"page_id\": {\n",
    "                        \"type\": \"resolve\",\n",
    "                        \"resource\": \"search\",\n",
    "                        \"field\": \"id\"\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "1. client: Here we added our base url, headers, and authentication\n",
    "\n",
    "2. resources: This is a list of endpoints that we wish to request data from (here: /search and /blocks/{page_id}/children)\n",
    "3. /search endpoint: The Notion API search endpoint allows us to filter pages based on the title. We can specify which pages we want returned based on the parameter \"query\". For example, if we'd like to return only those pages which has the word \"workshop\" in the title, then we would set  \"query\": \"workshop\" in the json body.\n",
    "\n",
    "\n",
    "As a response, it returns only page metadata (like page_id). Example response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "\"object\": \"list\",\n",
    "\"results\": [\n",
    "    {\n",
    "    \"object\": \"page\",\n",
    "    \"id\": \"954b67f9-3f87-41db-8874-23b92bbd31ee\",\n",
    "    \"created_time\": \"2022-07-06T19:30:00.000Z\",\n",
    "    \"last_edited_time\": \"2022-07-06T19:30:00.000Z\",\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "],\n",
    "\"next_cursor\": null,\n",
    "\"has_more\": false,\n",
    "\"type\": \"page_or_database\",\n",
    "\"page_or_database\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we would define our endpoint configuration for /search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"name\": \"search\",\n",
    "    \"endpoint\": {\n",
    "        \"path\": \"search\",\n",
    "        \"method\": \"POST\",\n",
    "        \"paginator\": PostBodyPaginator(),\n",
    "        \"json\": {\n",
    "            \"query\": \"workshop\",\n",
    "            \"sort\": {\n",
    "                \"direction\": \"ascending\",\n",
    "                \"timestamp\": \"last_edited_time\"\n",
    "            }\n",
    "        },\n",
    "        \"data_selector\": \"results\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* paginator allows us to specify the pagination strategy relevant for the API and the endpoint. (More on this later)\n",
    "* Since /search is a POST endpoint, we can include the json bodu inside the key kson.\n",
    "* We don't need the whole JSON response, but only the contents inside the field \"results\". We filter this out by specifying \"data_selector\":\"results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. blocks/{page_id}/children endpoint:\n",
    "\n",
    "* This is a GET point that reutns a list of block objects (in our case, paragraphs) from a specific page.\n",
    "* Since it accepts page_id as a parameter, we can pass this inside the key params\n",
    "* We would like to be able to automatically fethc the page_ids returned from the /search endpoint and pass it as parameter into the endpoint blocks/{page_id}/children. We can do this by linking the two resources as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"name\": \"page_content\",\n",
    "    \"endpoint\": {\n",
    "        \"path\": \"blocks/{page_id}/children\",\n",
    "        \"paginator\": JSONResponsePaginator(),\n",
    "        \"params\": {\n",
    "            \"page_id\": {\n",
    "                \"type\": \"resolve\",\n",
    "                \"resource\": \"search\",\n",
    "                \"field\": \"id\"\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying \"type\":\"resolve\", we are letting dlt know that this parameter needs to be resolved from the parent source \"search\" using the field \"id\", which corresponds to the page id in the response of /search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on pagination:\n",
    "\n",
    "Different REST APIs might use different strategies handle paginated responses. dlt has a built in support for most common pagination mechanisms, and these can be explicitly passed inside the configurarion like shown above.\n",
    "\n",
    "However inmost cases, it won't be necessary to explicitly specify the pagination strategy, since dlt detects it automatically.\n",
    "\n",
    "In case the sepecific pagination is no wupported by dlt yet, then you can also implement a custom paginator. For example, dlt does not have a buil-in paginator for POST methods, so we write our own paginator. We take the code provided in the docs for it and make small modification based on the notion API documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Request, Response\n",
    "\n",
    "\n",
    "class PosBodyPaginator(BasePaginator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cursor = None\n",
    "\n",
    "\n",
    "    def update_state(self, response: Response) -> None:\n",
    "        # Assuming the API returns and empty list when no more data is available\n",
    "        if not response.json():\n",
    "            self._has_next_page = False\n",
    "        else:\n",
    "            self.cursor = response.json().get(\"next_cursor\")\n",
    "            if self.cursor is None:\n",
    "                self._has_next_page = False\n",
    "        \n",
    "    def update_request(self, request: Request) -> None:\n",
    "        if request.json is None:\n",
    "            request.json = {}\n",
    "\n",
    "        request.json[\"start_cursor\"] = self.cursor\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Extract relevant content form the response body:\n",
    "\n",
    "    The response returned form the API is a nested JSON which we need to pre-process before using it anywhere, dlt can unnes json automatically, but since the Notion API is a little tricky it's better to pre-process this frist so we have a more structured DB as a result.\n",
    "\n",
    "    One ay to do this is to pass the JSON response thorugh a transformation function that will extract only the relevant data form the JSON body (we later add thus as a mapping to the resource):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "def extract_page_content(response):\n",
    "    block_id = response[\"id\"]\n",
    "    last_edited_time = response[\"last_edited_time\"]\n",
    "    block_type = response.get(\"type\", \"Not paragraph\")\n",
    "    if block_type != \"paragraph\":\n",
    "        content = \"\"\n",
    "    else:\n",
    "        try:\n",
    "            content = response['paragraph'][\"rich_text\"][0][\"plain_text\"]\n",
    "        except IndexError:\n",
    "            content = \"\"\n",
    "    return {\n",
    "       \"block_id\": block_id,\n",
    "       \"block_type\": block_type,\n",
    "       \"content\": content,\n",
    "       \"last_edited_time\": last_edited_time,\n",
    "       \"inserted_at_time\": datetime.now(timezone.utc)\n",
    "   }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also where you could implement some sort of chunking strategy, but we will omit this in this example as the Notion text is already pre-chunked into paragraphs. Any data pre-processing can also happen here.\n",
    "\n",
    "Note: If you want to include the parent page in the returned data, you can do so by including response[\"parent\"][\"page_id\"]. See 200 response example in the Notion docs.\n",
    "\n",
    "JSON response before the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "   \"object\": \"list\",\n",
    "   \"results\": [\n",
    "     {\n",
    "       \"object\": \"block\",\n",
    "       \"id\": \"c02fc1d3-db8b-45c5-a222-27595b15aea7\",\n",
    "       \"created_time\": \"2022-03-01T19:05:00.000Z\",\n",
    "       \"last_edited_time\": \"2022-03-01T19:05:00.000Z\",\n",
    "       .\n",
    "       .\n",
    "       .\n",
    "       \"type\": \"paragraph\",\n",
    "       \"paragraph\": {\n",
    "         \"rich_text\": [\n",
    "           {\n",
    "             .\n",
    "             .\n",
    "             .\n",
    "             \"annotations\": {\n",
    "               .\n",
    "               .\n",
    "               .\n",
    "\n",
    "             },\n",
    "             \"plain_text\": \"Lacinato kale is a variety of kale with a long tradition in Italian cuisine, especially that of Tuscany. It is also known as Tuscan kale, Italian kale, dinosaur kale, kale, flat back kale, palm tree kale, or black Tuscan palm.\",\n",
    "             \"href\": \"https://en.wikipedia.org/wiki/Lacinato_kale\"\n",
    "           }\n",
    "         ],\n",
    "         \"color\": \"default\"\n",
    "       }\n",
    "     }\n",
    "   ],\n",
    "   \"next_cursor\": null,\n",
    "   \"has_more\": false,\n",
    "   \"type\": \"block\",\n",
    "   \"block\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After passing it through the transformation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "   \"block_id\": \"c02fc1d3-db8b-45c5-a222-27595b15aea7\",\n",
    "   \"block_type\": \"paragraph\",\n",
    "   \"content\": \"Lacinato kale is a variety of kale with a long tradition in Italian cuisine, especially that of Tuscany. It is also known as Tuscan kale, Italian kale, dinosaur kale, kale, flat back kale, palm tree kale, or black Tuscan palm.\",\n",
    "   \"last_edited_time\": \"2022-03-01T19:05:00.000Z\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data incrementally\n",
    "\n",
    "Incremental loading is a very important aspect of building scalable data pipelines. It is the technique of loading only the new or changed data since the last run of the pipeline.\n",
    "\n",
    "In our case, when we first run the pipeline, all paragraphs from the employee handbook will get loaded as separate rows inside a lancedb table. Now if we change the content of one of the paragraphs and re-run the pipeline to update the table, then without doing incremental loading, one of two things may happen depending on the option we choose:\n",
    "\n",
    "If we choose the option \"replace\", then the existing data in lancedb will be dropped, and all of the paragraphs will be reloaded.\n",
    "If we choose the option \"append\", then the existing rows would remain and all of the paragraphs would be loaded again as new rows resulting in twice as many rows\n",
    "To ensure that only the new/changed rows are loaded we would need the following pieces:\n",
    "\n",
    "A column that can keep track of changes in the row (Example: only load rows where last_edited_time is greater than the current maximum last_edited_time)\n",
    "A primary_key column that uniquely identify a row, so it's possible to track when the row changes\n",
    "A strategy to resolve changes in a single row (example: drop current and load the changed row)\n",
    "This behaviour can be configured easily into a dlt rersource:\n",
    "\n",
    "Pass the incremental column as a parameter inside the resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rest_api_notion_incremental(\n",
    " last_edited_time = dlt.sources.incremental(\n",
    "        \"last_edited_time\", \n",
    "        initial_value=\"2024-06-26T08:16:00.000Z\",\n",
    "        primary_key=(\"block_id\")\n",
    "    )\n",
    "):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the column last_edited_time since it keeps track of whenever a paragraph changes.\n",
    "\n",
    "Pass the following arguments inside @dlt.resource to define the strategy for dealing with duplicate rows:\n",
    "\n",
    "* write_disposition=\"merge\": ensures that any duplicate rows are merged on the primary key\n",
    "* primary_key=\"block_id\": specifies the primary key that we'd like to merge on. In our case, this is block_id, which is a unique id corresponding to each block (paragraph).\n",
    "* columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}: this specifies the deduplication strategy (how we would like to resolve duplicate rows). Here we chose to keep the row with the largest value of last_edited_time.\n",
    "Putting it together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.resource(\n",
    "   name=\"employee_handbook\",\n",
    "   write_disposition=\"merge\",\n",
    "   primary_key=\"block_id\",\n",
    "   columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}\n",
    ")\n",
    "def rest_api_notion_incremental(\n",
    "    last_edited_time = dlt.sources.incremental(\n",
    "       \"last_edited_time\", \n",
    "       initial_value=\"2024-06-26T08:16:00.000Z\",\n",
    "       primary_key=(\"block_id\")\n",
    "    )\n",
    "):\n",
    "    for block in rest_api_notion_resource.add_map(extract_page_content):   \n",
    "        if not(len(block[\"content\"])):\n",
    "            continue\n",
    "        yield block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, rest_api_notion_resoure yields the JSON response from the Notion REST API and extract_page_content is the transformation function that we pass the JSON response through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create the pipeline and run it\n",
    "\n",
    "With our source configured, we can now define the pipeline and run it.\n",
    "\n",
    "Normally, to do this we would run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run(\n",
    " rest_api_notion_incremental,\n",
    " table_name=\"employee_handbook\",\n",
    " write_disposition=\"merge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this would load the data into lancedb normally, without creating any embeddings.\n",
    "\n",
    "However, we can have lancedb automatically create embeddings and load it along with the normal data using dlt's native adapter for lancedb: lancedb_adapter. It will use the embedding model that we specified in the credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run(\n",
    " lancedb_adapter(\n",
    "   rest_api_notion_incremental,\n",
    "   embed=\"content\" # The column that we'd like to embed\n",
    " )\n",
    " table_name=\"employee_handbook\",\n",
    " write_disposition=\"merge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline company_policies load step completed in 1.71 seconds\n",
      "1 load package(s) were loaded to destination LanceDB and into dataset notion_pages\n",
      "The LanceDB destination used <dlt.destinations.impl.lancedb.configuration.LanceDBCredentials object at 0x0000022B114D1B70> location to store data\n",
      "Load package 1721688414.8159938 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "class PostBodyPaginator(BasePaginator):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cursor = None\n",
    "\n",
    "    def update_state(self, response: Response) -> None:\n",
    "        # Assuming the API returns an empty list when no more data is available\n",
    "        if not response.json():\n",
    "            self._has_next_page = False\n",
    "        else:\n",
    "            self.cursor = response.json().get(\"next_cursor\")\n",
    "            if self.cursor is None:\n",
    "                self._has_next_page = False\n",
    "\n",
    "    def update_request(self, request: Request) -> None:\n",
    "        if request.json is None:\n",
    "            request.json = {}\n",
    "\n",
    "        # Add the cursor to the request body\n",
    "        request.json[\"start_cursor\"] = self.cursor\n",
    "\n",
    "@dlt.resource(name=\"employee_handbook\")\n",
    "def rest_api_notion_resource():\n",
    "    notion_config: RESTAPIConfig = {\n",
    "        \"client\": {\n",
    "            \"base_url\": \"https://api.notion.com/v1/\",\n",
    "            \"auth\": {\n",
    "                \"token\": dlt.secrets[\"sources.rest_api.notion.api_key\"]\n",
    "            },\n",
    "            \"headers\":{\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Notion-Version\": \"2022-06-28\"\n",
    "            }\n",
    "        },\n",
    "        \"resources\": [\n",
    "            {\n",
    "                \"name\": \"search\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"search\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"paginator\": PostBodyPaginator(),\n",
    "                    \"json\": {\n",
    "                        \"query\": \"Homework: Employee handbook\",\n",
    "                        \"sort\": {\n",
    "                            \"direction\": \"ascending\",\n",
    "                            \"timestamp\": \"last_edited_time\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"data_selector\": \"results\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page_content\",\n",
    "                \"endpoint\": {\n",
    "                    \"path\": \"blocks/{page_id}/children\",\n",
    "                    \"paginator\": JSONResponsePaginator(),\n",
    "                    \"params\": {\n",
    "                        \"page_id\": {\n",
    "                            \"type\": \"resolve\",\n",
    "                            \"resource\": \"search\",\n",
    "                            \"field\": \"id\"\n",
    "                        }\n",
    "                    },\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    yield from rest_api_source(notion_config,name=\"homework_employee_handbook\")\n",
    "\n",
    "def extract_page_content(response):\n",
    "    block_id = response[\"id\"]\n",
    "    last_edited_time = response[\"last_edited_time\"]\n",
    "    block_type = response.get(\"type\", \"Not paragraph\")\n",
    "    if block_type != \"paragraph\":\n",
    "        content = \"\"\n",
    "    else:\n",
    "        try:\n",
    "            content = response[\"paragraph\"][\"rich_text\"][0][\"plain_text\"]\n",
    "        except IndexError:\n",
    "            content = \"\"\n",
    "    return {\n",
    "        \"block_id\": block_id,\n",
    "        \"block_type\": block_type,\n",
    "        \"content\": content,\n",
    "        \"last_edited_time\": last_edited_time,\n",
    "        \"inserted_at_time\": datetime.now(timezone.utc)\n",
    "    }\n",
    "\n",
    "@dlt.resource(\n",
    "    name=\"employee_handbook\",\n",
    "    write_disposition=\"merge\",\n",
    "    primary_key=\"block_id\",\n",
    "    columns={\"last_edited_time\":{\"dedup_sort\":\"desc\"}}\n",
    "    )\n",
    "def rest_api_notion_incremental(\n",
    "    last_edited_time = dlt.sources.incremental(\"last_edited_time\", initial_value=\"2024-06-26T08:16:00.000Z\",primary_key=(\"block_id\"))\n",
    "):\n",
    "    # last_value = last_edited_time.last_value\n",
    "    # print(last_value)\n",
    "\n",
    "    for block in rest_api_notion_resource.add_map(extract_page_content):\n",
    "        if not(len(block[\"content\"])):\n",
    "            continue\n",
    "        yield block\n",
    "\n",
    "def load_notion() -> None:\n",
    "    pipeline = dlt.pipeline(\n",
    "        pipeline_name=\"company_policies\",\n",
    "        destination=\"lancedb\",\n",
    "        dataset_name=\"notion_pages\",\n",
    "        # full_refresh=True\n",
    "    )\n",
    "\n",
    "    load_info = pipeline.run(\n",
    "        lancedb_adapter(\n",
    "            rest_api_notion_incremental,\n",
    "            embed=\"content\"\n",
    "        ),\n",
    "        table_name=\"homework\",\n",
    "        write_disposition=\"merge\"\n",
    "    )\n",
    "    print(load_info)\n",
    "\n",
    "load_notion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\".lancedb\")\n",
    "dbtable = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "df = dbtable.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id__</th>\n",
       "      <th>vector__</th>\n",
       "      <th>block_id</th>\n",
       "      <th>block_type</th>\n",
       "      <th>content</th>\n",
       "      <th>last_edited_time</th>\n",
       "      <th>inserted_at_time</th>\n",
       "      <th>_dlt_load_id</th>\n",
       "      <th>_dlt_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c69f1ecf-7b02-5810-8286-3f42659ae9d4</td>\n",
       "      <td>[-0.02426561, 0.047460843, -0.011796447, 0.063...</td>\n",
       "      <td>a8196881-ae94-4767-8767-92fe1a327d24</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>We owe our success to our employees. To show o...</td>\n",
       "      <td>2024-07-05 22:34:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.115702+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>lHzqppPI6h8o5Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f2c18ac0-50f5-5b72-a871-dc5a46780353</td>\n",
       "      <td>[-0.04966159, 0.10853516, -0.009762599, -0.036...</td>\n",
       "      <td>31fcbf26-2ca5-468a-8af8-d7eb4c2db8c8</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>We want to ensure that private information abo...</td>\n",
       "      <td>2024-07-05 22:38:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.117701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>TyyBJdqQeH1oDg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4553193e-c655-54df-9a33-cfc570bf34d0</td>\n",
       "      <td>[-0.06316318, 0.17331503, 0.025351722, -0.0191...</td>\n",
       "      <td>da7721fd-3d0f-4c04-bc5e-825ad60bed1c</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Employee health is important to us. We don’t d...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.117701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>rdxWxBkCPFSuuw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>791be1a1-6c67-530d-87ab-bd9912500ea5</td>\n",
       "      <td>[-0.10974314, 0.105860785, 0.0032906178, -0.02...</td>\n",
       "      <td>ff36dcf3-5faa-40b4-ad8e-92fdc952201e</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Our company is dedicated to maintaining a safe...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.117701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>p01ya+QmSfjYnw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a83497f4-922c-5d62-bab1-53804e93c811</td>\n",
       "      <td>[0.052423313, -0.06457593, 0.065862976, 0.0145...</td>\n",
       "      <td>a1ff9697-4bb6-4f1e-b464-dda296dbd307</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>If your job doesn’t require you to be present ...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.118701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>h76zOAdnOpD8Bw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>434b71e9-a11a-519d-a9fe-e3ade78d47d6</td>\n",
       "      <td>[0.00052335917, -0.054883417, 0.043573372, -0....</td>\n",
       "      <td>e4ec9f4d-b687-4c28-a80d-985bfabcc2ba</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Remote working refers to working from a non-of...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.118701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>DpnGp1hJnnEK4w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17816363-54b7-5ba7-b8d5-06d871a25414</td>\n",
       "      <td>[0.038026277, -0.021509644, 0.04752782, 0.0647...</td>\n",
       "      <td>e6e550dc-b59e-4928-abd7-07eace948681</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>There are some expenses that we will pay direc...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.118701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>y9hFIVKsAPnR+g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2a434cf9-09d9-5514-a88b-02977f2f953e</td>\n",
       "      <td>[-0.058588095, -0.075404435, 0.03377518, 0.009...</td>\n",
       "      <td>a269d0ca-ce14-481b-a5f4-9192d6840d6e</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Our company operates between 9 a.m. to 7 p.m. ...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.118701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>VAn+vqbB0hgdLw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5f9384fa-7f98-5f52-a06e-05b05f42f69a</td>\n",
       "      <td>[-0.013599242, 0.0047530443, 0.024835145, 0.01...</td>\n",
       "      <td>5b65f3e7-0a37-429a-818d-f99b53755ebd</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>In this section, we are going to be covering i...</td>\n",
       "      <td>2024-07-05 23:33:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.118701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>r3JLk1B9PnpZEg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42af72f6-9db7-54a2-87b2-d466169078ff</td>\n",
       "      <td>[0.03206088, 0.024244627, 0.008471343, 0.03179...</td>\n",
       "      <td>b27f7d80-f2f1-460e-aa0c-b8e770cf050a</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Our company observes the following holidays: N...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.119700+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>GvRCdbpZeDKM/w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8bfc36ce-cdb0-5792-bd70-d12ff22fc227</td>\n",
       "      <td>[-0.013155303, 0.008382452, 0.017044418, 0.051...</td>\n",
       "      <td>cf2c7276-9d6d-4611-97ef-e7707a668176</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>These holidays are considered “off-days” for m...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.119700+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>LRljbcfEx+77qA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b1737a97-6dca-564c-a169-78e306a5e124</td>\n",
       "      <td>[0.027987482, 0.06734364, 0.039806444, 0.00774...</td>\n",
       "      <td>79fd88bb-894c-4db3-961e-f2e9fa571399</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Employees who are unable to work due to illnes...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.119700+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>kg1MolubtG6RqQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>896f7ed8-e918-54f3-a7b8-565d53d6d22b</td>\n",
       "      <td>[0.03252607, 0.008159488, 0.08443562, 0.055641...</td>\n",
       "      <td>0b5f3660-7867-41ea-ae25-95585c3bb34e</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Losing a loved one is traumatizing. If this ha...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.119700+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>2iyesG5IqKEcwQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3557e3dd-de5b-51b9-bdee-b53ba588ed60</td>\n",
       "      <td>[-0.007314066, 0.014710682, -0.01909119, 0.025...</td>\n",
       "      <td>3153e9cf-44bf-4ec1-b835-67d37731f9bc</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>In accordance with German law, we offer a comp...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.119700+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>ZpJ/b6nXCeXclw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>c51f99bf-5057-572e-bf9e-25dc03014c79</td>\n",
       "      <td>[-0.031538405, 0.034259953, -0.027282674, 0.02...</td>\n",
       "      <td>b9d67165-1028-4edc-841b-fe2fd4cf0cf7</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>We recognize the vital role that fathers and p...</td>\n",
       "      <td>2024-07-05 22:52:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.119700+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>Z2SExBmTE09AoQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0317522c-a6db-59e8-ba5f-5ff0dc960d2e</td>\n",
       "      <td>[-0.017366936, 0.060790606, 0.015769996, -0.01...</td>\n",
       "      <td>72d2f585-a1b4-461e-ad88-45d9e3346425</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>Our company’s official dress code is Business ...</td>\n",
       "      <td>2024-07-05 22:54:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.120701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>I2MzzOZztEaXQw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4d0eb4d7-f0ad-517c-8165-7eb7932ea0ed</td>\n",
       "      <td>[0.03365545, 0.035742387, 0.039310906, 0.00372...</td>\n",
       "      <td>b1a9a976-eef2-4c99-9f57-3e5adb873d1f</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>If you want to invite a visitor to our offices...</td>\n",
       "      <td>2024-07-05 22:56:00+00:00</td>\n",
       "      <td>2024-07-22 22:46:56.120701+00:00</td>\n",
       "      <td>1721688414.8159938</td>\n",
       "      <td>6kH4K6AYK1+RRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id__  \\\n",
       "0   c69f1ecf-7b02-5810-8286-3f42659ae9d4   \n",
       "1   f2c18ac0-50f5-5b72-a871-dc5a46780353   \n",
       "2   4553193e-c655-54df-9a33-cfc570bf34d0   \n",
       "3   791be1a1-6c67-530d-87ab-bd9912500ea5   \n",
       "4   a83497f4-922c-5d62-bab1-53804e93c811   \n",
       "5   434b71e9-a11a-519d-a9fe-e3ade78d47d6   \n",
       "6   17816363-54b7-5ba7-b8d5-06d871a25414   \n",
       "7   2a434cf9-09d9-5514-a88b-02977f2f953e   \n",
       "8   5f9384fa-7f98-5f52-a06e-05b05f42f69a   \n",
       "9   42af72f6-9db7-54a2-87b2-d466169078ff   \n",
       "10  8bfc36ce-cdb0-5792-bd70-d12ff22fc227   \n",
       "11  b1737a97-6dca-564c-a169-78e306a5e124   \n",
       "12  896f7ed8-e918-54f3-a7b8-565d53d6d22b   \n",
       "13  3557e3dd-de5b-51b9-bdee-b53ba588ed60   \n",
       "14  c51f99bf-5057-572e-bf9e-25dc03014c79   \n",
       "15  0317522c-a6db-59e8-ba5f-5ff0dc960d2e   \n",
       "16  4d0eb4d7-f0ad-517c-8165-7eb7932ea0ed   \n",
       "\n",
       "                                             vector__  \\\n",
       "0   [-0.02426561, 0.047460843, -0.011796447, 0.063...   \n",
       "1   [-0.04966159, 0.10853516, -0.009762599, -0.036...   \n",
       "2   [-0.06316318, 0.17331503, 0.025351722, -0.0191...   \n",
       "3   [-0.10974314, 0.105860785, 0.0032906178, -0.02...   \n",
       "4   [0.052423313, -0.06457593, 0.065862976, 0.0145...   \n",
       "5   [0.00052335917, -0.054883417, 0.043573372, -0....   \n",
       "6   [0.038026277, -0.021509644, 0.04752782, 0.0647...   \n",
       "7   [-0.058588095, -0.075404435, 0.03377518, 0.009...   \n",
       "8   [-0.013599242, 0.0047530443, 0.024835145, 0.01...   \n",
       "9   [0.03206088, 0.024244627, 0.008471343, 0.03179...   \n",
       "10  [-0.013155303, 0.008382452, 0.017044418, 0.051...   \n",
       "11  [0.027987482, 0.06734364, 0.039806444, 0.00774...   \n",
       "12  [0.03252607, 0.008159488, 0.08443562, 0.055641...   \n",
       "13  [-0.007314066, 0.014710682, -0.01909119, 0.025...   \n",
       "14  [-0.031538405, 0.034259953, -0.027282674, 0.02...   \n",
       "15  [-0.017366936, 0.060790606, 0.015769996, -0.01...   \n",
       "16  [0.03365545, 0.035742387, 0.039310906, 0.00372...   \n",
       "\n",
       "                                block_id block_type  \\\n",
       "0   a8196881-ae94-4767-8767-92fe1a327d24  paragraph   \n",
       "1   31fcbf26-2ca5-468a-8af8-d7eb4c2db8c8  paragraph   \n",
       "2   da7721fd-3d0f-4c04-bc5e-825ad60bed1c  paragraph   \n",
       "3   ff36dcf3-5faa-40b4-ad8e-92fdc952201e  paragraph   \n",
       "4   a1ff9697-4bb6-4f1e-b464-dda296dbd307  paragraph   \n",
       "5   e4ec9f4d-b687-4c28-a80d-985bfabcc2ba  paragraph   \n",
       "6   e6e550dc-b59e-4928-abd7-07eace948681  paragraph   \n",
       "7   a269d0ca-ce14-481b-a5f4-9192d6840d6e  paragraph   \n",
       "8   5b65f3e7-0a37-429a-818d-f99b53755ebd  paragraph   \n",
       "9   b27f7d80-f2f1-460e-aa0c-b8e770cf050a  paragraph   \n",
       "10  cf2c7276-9d6d-4611-97ef-e7707a668176  paragraph   \n",
       "11  79fd88bb-894c-4db3-961e-f2e9fa571399  paragraph   \n",
       "12  0b5f3660-7867-41ea-ae25-95585c3bb34e  paragraph   \n",
       "13  3153e9cf-44bf-4ec1-b835-67d37731f9bc  paragraph   \n",
       "14  b9d67165-1028-4edc-841b-fe2fd4cf0cf7  paragraph   \n",
       "15  72d2f585-a1b4-461e-ad88-45d9e3346425  paragraph   \n",
       "16  b1a9a976-eef2-4c99-9f57-3e5adb873d1f  paragraph   \n",
       "\n",
       "                                              content  \\\n",
       "0   We owe our success to our employees. To show o...   \n",
       "1   We want to ensure that private information abo...   \n",
       "2   Employee health is important to us. We don’t d...   \n",
       "3   Our company is dedicated to maintaining a safe...   \n",
       "4   If your job doesn’t require you to be present ...   \n",
       "5   Remote working refers to working from a non-of...   \n",
       "6   There are some expenses that we will pay direc...   \n",
       "7   Our company operates between 9 a.m. to 7 p.m. ...   \n",
       "8   In this section, we are going to be covering i...   \n",
       "9   Our company observes the following holidays: N...   \n",
       "10  These holidays are considered “off-days” for m...   \n",
       "11  Employees who are unable to work due to illnes...   \n",
       "12  Losing a loved one is traumatizing. If this ha...   \n",
       "13  In accordance with German law, we offer a comp...   \n",
       "14  We recognize the vital role that fathers and p...   \n",
       "15  Our company’s official dress code is Business ...   \n",
       "16  If you want to invite a visitor to our offices...   \n",
       "\n",
       "            last_edited_time                 inserted_at_time  \\\n",
       "0  2024-07-05 22:34:00+00:00 2024-07-22 22:46:56.115702+00:00   \n",
       "1  2024-07-05 22:38:00+00:00 2024-07-22 22:46:56.117701+00:00   \n",
       "2  2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.117701+00:00   \n",
       "3  2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.117701+00:00   \n",
       "4  2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.118701+00:00   \n",
       "5  2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.118701+00:00   \n",
       "6  2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.118701+00:00   \n",
       "7  2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.118701+00:00   \n",
       "8  2024-07-05 23:33:00+00:00 2024-07-22 22:46:56.118701+00:00   \n",
       "9  2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.119700+00:00   \n",
       "10 2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.119700+00:00   \n",
       "11 2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.119700+00:00   \n",
       "12 2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.119700+00:00   \n",
       "13 2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.119700+00:00   \n",
       "14 2024-07-05 22:52:00+00:00 2024-07-22 22:46:56.119700+00:00   \n",
       "15 2024-07-05 22:54:00+00:00 2024-07-22 22:46:56.120701+00:00   \n",
       "16 2024-07-05 22:56:00+00:00 2024-07-22 22:46:56.120701+00:00   \n",
       "\n",
       "          _dlt_load_id         _dlt_id  \n",
       "0   1721688414.8159938  lHzqppPI6h8o5Q  \n",
       "1   1721688414.8159938  TyyBJdqQeH1oDg  \n",
       "2   1721688414.8159938  rdxWxBkCPFSuuw  \n",
       "3   1721688414.8159938  p01ya+QmSfjYnw  \n",
       "4   1721688414.8159938  h76zOAdnOpD8Bw  \n",
       "5   1721688414.8159938  DpnGp1hJnnEK4w  \n",
       "6   1721688414.8159938  y9hFIVKsAPnR+g  \n",
       "7   1721688414.8159938  VAn+vqbB0hgdLw  \n",
       "8   1721688414.8159938  r3JLk1B9PnpZEg  \n",
       "9   1721688414.8159938  GvRCdbpZeDKM/w  \n",
       "10  1721688414.8159938  LRljbcfEx+77qA  \n",
       "11  1721688414.8159938  kg1MolubtG6RqQ  \n",
       "12  1721688414.8159938  2iyesG5IqKEcwQ  \n",
       "13  1721688414.8159938  ZpJ/b6nXCeXclw  \n",
       "14  1721688414.8159938  Z2SExBmTE09AoQ  \n",
       "15  1721688414.8159938  I2MzzOZztEaXQw  \n",
       "16  1721688414.8159938  6kH4K6AYK1+RRA  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2024-07-05 23:33:00+0000', tz='UTC')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.last_edited_time.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make change to one of the paragraphs and run the pipeline again to see the effect of incremental loading. We observe two things:\n",
    "\n",
    "1. The column inserted_at_time only changed for the updated row, implying that only this row was added\n",
    "2. Looking at the primary key block_id we see that the original row was dropped and the updated row was inserted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create a RAG bot using Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the contents from the employee handbook vectorized and stored in LanceDB, we're now ready to create our RAG with Ollama.\n",
    "\n",
    "What is RAG?\n",
    "\n",
    "Retrieval Automated Generation (RAG) is the framework of retrieving relevant documents from a database and passing it along with a query into an LLM so that the LLM can generate context-aware responses.\n",
    "\n",
    "In our case, if we were to ask an LLM questions about our specific employee policies, then we would not get useful responses because the LLM has never seen these policies. A solution to this could be to paste all of the policies into the prompt and then ask our questions. However, this would not be feasible given the limitations on the size of the context window.\n",
    "\n",
    "We can bypass this limitation using RAG:\n",
    "\n",
    "Given a user question, we would first embed this question into a vector\n",
    "Then we would do a vector search on our LanceDB table and retrieve top k results - which would be the most relevant paragraphs corresponding to the question\n",
    "Finally, we would pass the original question along with the retrieved paragraphs as a prompt into the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install Ollama into the notebook's local runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Start Ollama using ollama serve. This needs to run in the backgound - so we run it using nohup (to see the output log, open nohup.out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Versi�n 10.0.19045.4651]\n",
      "(c) Microsoft Corporation. Todos los derechos reservados.\n",
      "\n",
      "(llm-zoomcampt-3.10) c:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\homeworks>nohup ollama serve > nohup.out 2>&1 &\n",
      "\n",
      "(llm-zoomcampt-3.10) c:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\homeworks>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "nohup ollama serve > nohup.out 2>&1 &\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pull the desired model. We're going to be using llama1-uncensored (takes about 1m to download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Versi�n 10.0.19045.4651]\n",
      "(c) Microsoft Corporation. Todos los derechos reservados.\n",
      "\n",
      "(llm-zoomcampt-3.10) c:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\homeworks>ollama pull llama2-uncensored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 6aa74acf170f... 100% ▕████████████████▏ 3.8 GB                         \n",
      "pulling 8c17c2ebb0ea... 100% ▕████████████████▏ 7.0 KB                         \n",
      "pulling 7c23fb36d801... 100% ▕████████████████▏ 4.8 KB                         \n",
      "pulling 28577ba2177f... 100% ▕████████████████▏   55 B                         \n",
      "pulling 0025f348941e... 100% ▕████████████████▏   39 B                         \n",
      "pulling c67e365e770d... 100% ▕████████████████▏  529 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "removing any unused layers \n",
      "success \u001b[?25h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(llm-zoomcampt-3.10) c:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\homeworks>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "ollama pull llama2-uncensored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this next part we're going to be writing functions that accept user question, retrieve the relevant paragraphs from lancedb, and the pass the question and the retrieved pages as input into the ollama chat assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\user\\documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.2.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\user\\documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (1.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1 in c:\\users\\user\\documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama) (4.12.0)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Write a function that can retrieve content from lancedb relevant to the user query\n",
    "  \n",
    "  With LanceDB, you don't have to explicity embed the question. LanceDB stores information on the embedding model used and automatically embeds the question.\n",
    "\n",
    "  We use the `db_table.search()` function to query the DB and then limit it to the top 2 most similar results and return that as the context to pass to the RAG.\n",
    "\n",
    "  Limiting results is important because otherwise there might be too much confusing information. Similarly only picking the top choice might not give enough information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_from_lancedb(dbtable, question, top_k=2):\n",
    "\n",
    "    query_results = dbtable.search(query=question).to_list()\n",
    "    context = \"\\n\".join([result[\"content\"] for result in query_results[:top_k]])\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Finally we define a very basic RAG. We define a simple system prompt, retrieve the relevant context for the user query with the function defined above and then send the user question and the context to the `llama2-uncensored` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "def main():\n",
    "  # Connect to the lancedb table\n",
    "  db = lancedb.connect(\".lancedb\")\n",
    "  dbtable = db.open_table(\"notion_pages___homework\")\n",
    "\n",
    "  # A system prompt telling ollama to accept input in the form of \"Question: ... ; Context: ...\"\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps users understand policies inside a company's employee handbook. The user will first ask you a question and then provide you relevant paragraphs from the handbook as context. Please answer the question based on the provided context. For any details missing in the paragraph, encourage the employee to contact the HR for that information. Please keep the responses conversational.\"}\n",
    "  ]\n",
    "\n",
    "  while True:\n",
    "    # Accept user question\n",
    "    question = input(\"You: \")\n",
    "\n",
    "    # Retrieve the relevant paragraphs on the question\n",
    "    context = retrieve_context_from_lancedb(dbtable,question,top_k=2)\n",
    "\n",
    "    # Create a user prompt using the question and retrieved context\n",
    "    messages.append(\n",
    "        {\"role\": \"user\", \"content\": f\"Question: '{question}'; Context:'{context}'\"}\n",
    "    )\n",
    "\n",
    "    # Get the response from the LLM\n",
    "    response = ollama.chat(\n",
    "        model=\"llama2-uncensored\",\n",
    "        messages=messages\n",
    "    )\n",
    "    response_content = response['message']['content']\n",
    "    print(f\"Assistant: {response_content}\")\n",
    "\n",
    "    # Add the response into the context window\n",
    "    messages.append(\n",
    "        {\"role\": \"assistant\", \"content\":response_content}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we run the RAG! Some example questions you can ask:\n",
    "\n",
    "* How many vacation days do I get?\n",
    "* Can I get maternity leave?\n",
    "\n",
    "**Note**: This is a very basic implementation of a RAG, since this workshop is mainly about data ingestion. So expect some weird answers. If you do stop and restart the cell, you will need to rerun the cell containing `ollama serve` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hi there! I hope you are doing well today. Let me help you with this question. According to the information provided, employees receive 30 days of Paid Time Off (PTO) per year. Based on that context, your answer is thirty days. If the employee wishes to check the company's policy about taking more than 30 days of PTO in a year, they should contact their HR department or read through the employee handbook for further information.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 25\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m     21\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; Context:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Get the response from the LLM\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama2-uncensored\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m response_content \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\ollama\\_client.py:235\u001b[0m, in \u001b[0;36mClient.chat\u001b[1;34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[0m\n\u001b[0;32m    232\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;241m:=\u001b[39m message\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    233\u001b[0m     message[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [_encode_image(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/api/chat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m  \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkeep_alive\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m  \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\ollama\\_client.py:98\u001b[0m, in \u001b[0;36mClient._request_stream\u001b[1;34m(self, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request_stream\u001b[39m(\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m     95\u001b[0m   stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     96\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     97\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], Iterator[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[1;32m---> 98\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\ollama\\_client.py:69\u001b[0m, in \u001b[0;36mClient._request\u001b[1;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m httpx\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[1;32m---> 69\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     71\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpx\\_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    826\u001b[0m )\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    910\u001b[0m )\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpx\\_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpx\\_transports\\default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    231\u001b[0m )\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    242\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpcore\\_sync\\http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpcore\\_sync\\http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    107\u001b[0m     (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m         trailing_data,\n\u001b[1;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m         http_version,\n\u001b[0;32m    116\u001b[0m         status,\n\u001b[0;32m    117\u001b[0m         reason_phrase,\n\u001b[0;32m    118\u001b[0m         headers,\n\u001b[0;32m    119\u001b[0m     )\n\u001b[0;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpcore\\_sync\\http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpcore\\_sync\\http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32mc:\\Users\\User\\Documents\\llm-zoomcamp-dbeta95\\llm-zoomcampt-3.10\\lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-zoomcampt-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
